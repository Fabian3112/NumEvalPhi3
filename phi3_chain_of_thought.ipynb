{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chaine_of_thought_examples.json','r') as f:\n",
    "    examples = json.load(f)\n",
    "\n",
    "template = '{question} Option 1 : {Option1} or Option 2 : {Option2}. lets think step by step. {answer} \\n \\n '\n",
    "\n",
    "text = \"\"\n",
    "for example in examples:\n",
    "    text += template.format(question=example['question'],Option1=example['Option1'],Option2=example['Option2'],answer=example['answer'])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'models/phi3'#\"microsoft/Phi-3-medium-128k-instruct\"\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    #attn_implementation=\"flash_attention_2\",  # loading the model with flash-attenstion support\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path)#, **model_kwargs) #Uncomment for download\n",
    "checkpoint_path = 'tokenizer/phi3'#\"microsoft/Phi-3-medium-128k-instruct\" #\"tokenizer/phi3/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 512#2048\n",
    "tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'\n",
    "model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_Dataset(path,tokenizer) -> Dataset:\n",
    "    system_message = 'you are an AI assistant trained to answere Questions. You will decide wether \" Option 1 \" or \" Option 2 \" is correct. Think step by step'\n",
    "    \n",
    "    with open(path,'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open('chaine_of_thought_examples.json','r') as f:\n",
    "        examples = json.load(f)\n",
    "\n",
    "    data_dict = {'messages':[],'labels':[]}\n",
    "    for item in data:\n",
    "        \n",
    "        messages = [{\"role\":\"system\", \"content\": \" \" + system_message }]\n",
    "        template = '{question} Option 1 : {Option1} or Option 2 : {Option2}. lets think step by step.'\n",
    "        for example in examples:\n",
    "            question = template.format(question=example['question'],Option1=example['Option1'],Option2=example['Option2'])\n",
    "            messages.append({\"role\": \"user\", \"content\": \" \" + question})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": \" \" + example['answer']})\n",
    "        #for i in range(2):\n",
    "        #    example= data[np.random.randint(0, len(data))]\n",
    "        #    example_question = example['question'] + ' Option 1 : ' + example['Option1'] + ' Option 2 : ' + example['Option2'] + ' lets think step by step'\n",
    "        #    messages.append({\"role\": \"user\", \"content\": \" \" + example_question})\n",
    "        #    messages.append({\"role\": \"assistant\", \"content\": \" The answer is: \" + example['answer']})\n",
    "\n",
    "        question = template.format(question=item['question'],Option1=item['Option1'],Option2=item['Option2']) #item['question'] + ' Option 1 : ' + item['Option1'] + ' Option 2 : ' + item['Option2'] + ' lets think step by step'\n",
    "        #messages = [{\"role\":\"system\", \"content\": \" \" + system_message },{\"role\": \"user\", \"content\": \" \" + question},{\"role\": \"assistant\", \"content\": \" \" + answer}]\n",
    "        messages.append({\"role\": \"user\", \"content\": \" \" + question})\n",
    "        #messages.append({\"role\": \"assistant\", \"content\": \" The answer is: \" + answer})\n",
    "        input = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "        data_dict['messages'].append(input)\n",
    "        data_dict['labels'].append(item[\"answer\"])\n",
    "\n",
    "        # add same question reversed Options\n",
    "        #question = item['question'] + ' Option 1 : ' + item['Option2'] + ' Option 2 : ' + item['Option1']\n",
    "        #messages = [{\"role\":\"system\", \"content\":\" \" + system_message },{\"role\": \"user\", \"content\": \" \" + question},{\"role\": \"assistant\", \"content\": \" \" + answer_reversed}]\n",
    "        #input = tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
    "        #data_dict['messages'].append(input)\n",
    "    print(np.max([len(m) for m in data_dict['messages'] ]))\n",
    "    return  Dataset.from_dict(data_dict) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(\n",
    "    example,\n",
    "    tokenizer,\n",
    "):\n",
    "    messages = example[\"messages\"]\n",
    "    example[\"text\"] = messages#tokenizer.apply_chat_template(\n",
    "        #messages, tokenize=False, add_generation_prompt=False)\n",
    "    return example\n",
    "\n",
    "train_dataset = make_Dataset('data/QQA/QQA_train.json',tokenizer)\n",
    "test_dataset = make_Dataset('data/QQA/QQA_dev.json',tokenizer)\n",
    "column_names =  list(['messages'])\n",
    "\n",
    "processed_train_dataset = train_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    num_proc=10,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Applying chat template to train_sft\",\n",
    ")\n",
    "\n",
    "processed_test_dataset = test_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    num_proc=10,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Applying chat template to test_sft\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(index):\n",
    "    input = tokenizer.encode(processed_train_dataset[index][\"text\"],return_tensors=\"pt\", max_length=4096)\n",
    "    input = input.to('mps')\n",
    "    output = model.generate(input, max_new_tokens=512)\n",
    "    output_text = tokenizer.batch_decode(output)[0]\n",
    "    print('\\n \\n----------------------------')\n",
    "    print(\"output Text: /n\" + output_text)\n",
    "    prediction = output_text.split(\"<|assistant|>\")[-1]\n",
    "    print(\"\\n \\n predicted answer : \\n\" + prediction)\n",
    "    answer = processed_train_dataset[index][\"labels\"]\n",
    "    print(\"\\n \\n the answer was : \\n\" + answer)\n",
    "\n",
    "    if answer in prediction:\n",
    "        print(\"correct predited\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_count = 0\n",
    "total_count = 0\n",
    "for index,_ in enumerate(processed_train_dataset):\n",
    "    res = predict(index)\n",
    "\n",
    "    total_count += 1\n",
    "    if res:\n",
    "        true_count += 1\n",
    "\n",
    "    print(f'{total_count} -- {total_count/len(processed_train_dataset)} -- accuracy : {true_count / total_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
