{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfdcb5a2-059b-44f5-8ee8-a0646d11178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import random, os\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cell 2: Define tool functions (從 util.py)\n",
    "def read_jsonl(path: str):\n",
    "    with open(path) as fh:\n",
    "        return [json.loads(line) for line in fh.readlines() if line]\n",
    "\n",
    "def remove_key_json(json_data, key_to_remove):\n",
    "    return [{key: value for key, value in data.items() if key not in key_to_remove} for data in json_data]\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.enabled = False \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Cell 3: Set up starting arguments\n",
    "args = {\n",
    "    \"data_train_pth\": './Quantitative-101/QQA/QQA_train.json',\n",
    "    \"data_dev_pth\": './Quantitative-101/QQA/QQA_dev.json',\n",
    "    \"data_test_pth\": \"./Quantitative-101/QQA/QQA_test.json\",\n",
    "    \"is_digit_base\": False,\n",
    "    \"has_demonstrations\": True,\n",
    "    \"model_name\": 'google/flan-t5-base',\n",
    "    \"seed\": 33,\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\"\n",
    "}\n",
    "\n",
    "# 設置隨機種子\n",
    "set_seed(args[\"seed\"])\n",
    "\n",
    "# Cell 4: Load data\n",
    "train_data = read_jsonl(args[\"data_train_pth\"])\n",
    "dev_data = read_jsonl(args[\"data_dev_pth\"])\n",
    "test_data = read_jsonl(args[\"data_test_pth\"])\n",
    "\n",
    "# # 添加採樣比例參數\n",
    "# sample_ratio = 0.01  # 使用1%的數據進行測試\n",
    "# sample_size_train = int(len(train_data[0]) * sample_ratio)\n",
    "# sample_size_dev = int(len(dev_data[0]) * sample_ratio)\n",
    "# sample_size_test = int(len(test_data[0]) * sample_ratio)\n",
    "\n",
    "# # 隨機採樣\n",
    "# train_data = [random.sample(train_data[0], sample_size_train)]\n",
    "# dev_data = [random.sample(dev_data[0], sample_size_dev)]\n",
    "# test_data = [random.sample(test_data[0], sample_size_test)]\n",
    "\n",
    "# Cell 5: Data Type Conversion\n",
    "def trans_to_dict_qqa(data):\n",
    "    # 移除不需要的欄位\n",
    "    data = remove_key_json(data, ['type', 'question_sci_10E', 'question_sci_10E_char', 'question_mask'])\n",
    "    \n",
    "    # 初始化字典\n",
    "    keys = data[0].keys()\n",
    "    data_dic = {}\n",
    "    \n",
    "    for key in keys:\n",
    "        data_dic[key] = []\n",
    "    \n",
    "    # 轉換數據\n",
    "    for item in data:\n",
    "        for key in keys:\n",
    "            sstr = item[key]\n",
    "            sstr = str(sstr)\n",
    "            data_dic[key].append(sstr.strip())\n",
    "    \n",
    "    return data_dic\n",
    "\n",
    "# 轉換訓練和驗證數據\n",
    "train_dict = trans_to_dict_qqa(train_data[0])\n",
    "dev_dict = trans_to_dict_qqa(dev_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c626d357-11a4-4761-ae2e-ad8493650fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可用的欄位： ['question', 'Option1', 'Option2', 'answer', 'question_char']\n",
      "\n",
      "訓練數據大小： 564\n",
      "驗證數據大小： 81\n",
      "\n",
      "第一個訓練樣本：\n",
      "question: The ranger and the rustler both were riding horses that galloped at the same speed.  The rustler left at 01:00 where as the ranger left at 0500 hours. Who has traveled further??\n",
      "Option1: the ranger\n",
      "Option2: the rustler\n",
      "answer: Option 2\n",
      "question_char: The ranger and the rustler both were riding horses that galloped at the same speed.  The rustler left at 0 1:0 0 where as the ranger left at 050 0 hours. Who has traveled further??\n"
     ]
    }
   ],
   "source": [
    "# 看看處理後的數據結構\n",
    "print(\"可用的欄位：\", list(train_dict.keys()))\n",
    "print(\"\\n訓練數據大小：\", len(train_dict['question']))\n",
    "print(\"驗證數據大小：\", len(dev_dict['question']))\n",
    "\n",
    "# 看看一個樣本的內容\n",
    "print(\"\\n第一個訓練樣本：\")\n",
    "for key in train_dict.keys():\n",
    "    print(f\"{key}: {train_dict[key][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746c36bb-0e66-4c42-b155-e1bbf3b5c249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模板示例：\n",
      "Choose a correct answer to the following questions\n",
      "Question: Rolling a marble over dirt creates 1.2 mega N resistance, whereas rolling it over sand creates 45 N resistance. This means the marble will travel further over the?\n",
      "Option 1: sand\n",
      "Option 2: dirt\n",
      "Answer: Option 1\n",
      "Choose a correct answer to the following questions\n",
      "Question: A toddler is rolling a ball for more than 1 mins on the grass and rolls it on to the sand where it stops after 43 seconds. The sand stopped the ball because it has _____ than the grass.?\n",
      "Option 1: more friction\n",
      "Option 2: less friction\n",
      "Answer: Option 1\n",
      "Choose a correct answer to the following questions\n",
      "Question: Marlo weighs 678 N whereas his friend Dan weighs 852 N . The person which has more mass is likely? \n",
      "Option 1: Marlo\n",
      "Option 2: Dan\n",
      "Answer: Option 2\n",
      "Choose a correct answer to the following questions\n",
      "Question: The F-16 usually weighs 9034 kg and the jumbo jet weighs 439987 kg. Therefore, the F-16 was? \n",
      "Option 1: slower accelerating\n",
      "Option 2: faster accelerating\n",
      "Answer: Option 2\n",
      "Choose a correct answer to the following questions\n",
      "Question: The ranger and the rustler both were riding horses that galloped at the same speed.  The rustler left at 01:00 where as the ranger left at 0500 hours. Who has traveled further??\n",
      "Option 1: the ranger\n",
      "Option 2: the rustler\n"
     ]
    }
   ],
   "source": [
    "class instr_template:\n",
    "    def __init__(self):\n",
    "        self.input_template = {}\n",
    "    \n",
    "    def load_qqa_template(self):\n",
    "        # 從 instruction_config.py 複製模板\n",
    "        self.input_template['icl'] = f\"\"\"Choose a correct answer to the following questions\n",
    "Question: Rolling a marble over dirt creates 1.2 mega N resistance, whereas rolling it over sand creates 45 N resistance. This means the marble will travel further over the?\n",
    "Option 1: sand\n",
    "Option 2: dirt\n",
    "Answer: Option 1\n",
    "Choose a correct answer to the following questions\n",
    "Question: A toddler is rolling a ball for more than 1 mins on the grass and rolls it on to the sand where it stops after 43 seconds. The sand stopped the ball because it has _____ than the grass.?\n",
    "Option 1: more friction\n",
    "Option 2: less friction\n",
    "Answer: Option 1\n",
    "Choose a correct answer to the following questions\n",
    "Question: Marlo weighs 678 N whereas his friend Dan weighs 852 N . The person which has more mass is likely? \n",
    "Option 1: Marlo\n",
    "Option 2: Dan\n",
    "Answer: Option 2\n",
    "Choose a correct answer to the following questions\n",
    "Question: The F-16 usually weighs 9034 kg and the jumbo jet weighs 439987 kg. Therefore, the F-16 was? \n",
    "Option 1: slower accelerating\n",
    "Option 2: faster accelerating\n",
    "Answer: Option 2\n",
    "Choose a correct answer to the following questions\n",
    "Question: {{question}}\n",
    "Option 1: {{option1}}\n",
    "Option 2: {{option2}}\"\"\"\n",
    "        self.input_template['instr'] = f\"\"\"Choose a correct answer to the following questions\n",
    "Question: {{question}}\n",
    "Option 1: {{option1}}\n",
    "Option 2: {{option2}}\"\"\"\n",
    "\n",
    "qqa_template = instr_template()\n",
    "qqa_template.load_qqa_template()\n",
    "\n",
    "if args[\"has_demonstrations\"]:\n",
    "    input_template = qqa_template.input_template['icl']\n",
    "else:\n",
    "    input_template = qqa_template.input_template['instr']\n",
    "\n",
    "print(\"模板示例：\")\n",
    "print(input_template.format(question=train_dict['question'][0], option1=train_dict['Option1'][0], option2=train_dict['Option2'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af672c53-9ec1-459b-be9a-e8a24c7cc971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/miniconda3/envs/semeval/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd59ad10658e4bcd835711386baab77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a2e7fb1b50498ca65926dfcfe6eb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "處理後的數據結構：\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 564\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 81\n",
      "    })\n",
      "})\n",
      "\n",
      "處理後的第一個樣本：\n",
      "輸入 ID: [7023, 3, 9, 2024, 1525, 12, 8, 826, 746, 11860, 10, 6070, 53, 3, 9, 14260, 147, 9404, 482, 7, 3, 10917, 13950, 445, 5673, 6, 3, 10339, 8394, 34, 147, 3, 7, 232, 482, 7, 3479, 445, 5673, 5, 100, 598, 8, 14260, 56, 1111, 856, 147, 8, 58, 10231, 209, 10, 3, 7, 232, 10231, 204, 10, 9404, 11801, 10, 10231, 209, 7023, 3, 9, 2024, 1525, 12, 8, 826, 746, 11860, 10, 71, 13817, 19, 8394, 3, 9, 1996, 21, 72, 145, 209, 3519, 7, 30, 8, 5956, 11, 15246, 34, 30, 12, 8, 3, 7, 232, 213, 34, 10796, 227, 8838, 3978, 5, 37, 3, 7, 232, 4910, 8, 1996, 250, 34, 65, 31020, 145, 8, 5956, 5, 58, 10231, 209, 10, 72, 21764, 10231, 204, 10, 705, 21764, 11801, 10, 10231, 209, 7023, 3, 9, 2024, 1525, 12, 8, 826, 746, 11860, 10, 1571, 40, 32, 11385, 7, 431, 3940, 445, 3, 10339, 112, 1565, 2744, 11385, 7, 505, 5373, 445, 3, 5, 37, 568, 84, 65, 72, 3294, 19, 952, 58, 10231, 209, 10, 1571, 40, 32, 10231, 204, 10, 2744, 11801, 10, 10231, 204, 7023, 3, 9, 2024, 1525, 12, 8, 826, 746, 11860, 10, 37, 377, 10892, 1086, 11385, 7, 2777, 3710, 9147, 11, 8, 3, 2047, 6310, 8757, 11385, 7, 8838, 3264, 4225, 9147, 5, 4063, 6, 8, 377, 10892, 47, 58, 10231, 209, 10, 17553, 3, 30819, 10231, 204, 10, 3627, 3, 30819, 11801, 10, 10231, 204, 7023, 3, 9, 2024, 1525, 12, 8, 826, 746, 11860, 10, 37, 620, 52, 11, 8, 3, 9277, 1171, 321, 130, 7494, 10235, 24, 12486, 32, 3138, 44, 8, 337, 1634, 5, 37, 3, 9277, 1171, 646, 44, 3, 632, 24294, 213, 38, 8, 620, 52, 646, 44, 3, 632, 2560, 716, 5, 2645, 65, 15458, 856, 8546, 10231, 209, 10, 8, 620, 52, 10231, 204, 10, 8, 3, 9277, 1171, 1]\n",
      "標籤: [10231, 204, 10, 8, 3, 9277, 1171, 1]\n",
      "\n",
      "解碼後的文本：\n",
      "輸入文本: Choose a correct answer to the following questions Question: Rolling a marble over dirt creates 1.2 mega N resistance, whereas rolling it over sand creates 45 N resistance. This means the marble will travel further over the? Option 1: sand Option 2: dirt Answer: Option 1 Choose a correct answer to the following questions Question: A toddler is rolling a ball for more than 1 mins on the grass and rolls it on to the sand where it stops after 43 seconds. The sand stopped the ball because it has _____ than the grass.? Option 1: more friction Option 2: less friction Answer: Option 1 Choose a correct answer to the following questions Question: Marlo weighs 678 N whereas his friend Dan weighs 852 N. The person which has more mass is likely? Option 1: Marlo Option 2: Dan Answer: Option 2 Choose a correct answer to the following questions Question: The F-16 usually weighs 9034 kg and the jumbo jet weighs 439987 kg. Therefore, the F-16 was? Option 1: slower accelerating Option 2: faster accelerating Answer: Option 2 Choose a correct answer to the following questions Question: The ranger and the rustler both were riding horses that galloped at the same speed. The rustler left at 01:00 where as the ranger left at 0500 hours. Who has traveled further?? Option 1: the ranger Option 2: the rustler</s>\n",
      "標籤: Option 2: the rustler</s>\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: 設置 tokenizer 和預處理函數\n",
    "tokenizer = AutoTokenizer.from_pretrained(args[\"model_name\"])\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # 準備輸入\n",
    "    inputs = [input_template.format(\n",
    "        question=question,\n",
    "        option1=option1,\n",
    "        option2=option2\n",
    "    ) for question, option1, option2 in zip(\n",
    "        examples['question'],\n",
    "        examples[\"Option1\"],\n",
    "        examples[\"Option2\"]\n",
    "    )]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, truncation=True, max_length=512)\n",
    "    \n",
    "    # 準備標籤\n",
    "    labels = []\n",
    "    for answer, option1, option2 in zip(examples[\"answer\"], examples['Option1'], examples['Option2']):\n",
    "        if '1' in answer:\n",
    "            labels.append(answer+\": \"+option1)\n",
    "        elif '2' in answer:\n",
    "            labels.append(answer+\": \"+option2)\n",
    "    \n",
    "    model_labels = tokenizer(text_target=labels, truncation=True)\n",
    "    model_inputs[\"labels\"] = model_labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# 創建數據集\n",
    "datasets = DatasetDict({\n",
    "    'train': Dataset.from_dict(train_dict),\n",
    "    'validation': Dataset.from_dict(dev_dict)\n",
    "})\n",
    "\n",
    "# 對數據集進行預處理\n",
    "tokenized_datasets = datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# 檢查處理後的數據\n",
    "print(\"\\n處理後的數據結構：\")\n",
    "print(tokenized_datasets)\n",
    "\n",
    "# 查看一個處理後的樣本\n",
    "print(\"\\n處理後的第一個樣本：\")\n",
    "print(\"輸入 ID:\", tokenized_datasets[\"train\"][0][\"input_ids\"])\n",
    "print(\"標籤:\", tokenized_datasets[\"train\"][0][\"labels\"])\n",
    "\n",
    "# 解碼檢查\n",
    "print(\"\\n解碼後的文本：\")\n",
    "print(\"輸入文本:\", tokenizer.decode(tokenized_datasets[\"train\"][0][\"input_ids\"]))\n",
    "print(\"標籤:\", tokenizer.decode(tokenized_datasets[\"train\"][0][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab0f7b57-232e-46f5-9989-3b1d33757427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: 設置訓練參數\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results_qqa\",\n",
    "    evaluation_strategy=args[\"evaluation_strategy\"],\n",
    "    save_strategy=args[\"save_strategy\"],\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    predict_with_generate=True,\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "f1_metric = evaluate.load(\"./subtask1/f1.py\")\n",
    "# Cell 8: 定義評估指標\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # 轉換為 0/1 標籤\n",
    "    decoded_preds = [0 if item.startswith(\"Option 1\") else 1 for item in decoded_preds]\n",
    "    decoded_labels = [0 if item.startswith(\"Option 1\") else 1 for item in decoded_labels]\n",
    "\n",
    "    # 計算 F1 分數\n",
    "    macro_f1 = f1_metric.compute(predictions=decoded_preds, references=decoded_labels, average=\"macro\")\n",
    "    micro_f1 = f1_metric.compute(predictions=decoded_preds, references=decoded_labels, average=\"micro\")\n",
    "\n",
    "    return {\n",
    "        'macro_f1': macro_f1['f1']*100,\n",
    "        'micro_f1': micro_f1['f1']*100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "851db052-2d56-4410-bd48-1bb35847635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/miniconda3/envs/semeval/lib/python3.9/site-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "/home/k/miniconda3/envs/semeval/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始訓練...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/miniconda3/envs/semeval/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='710' max='710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [710/710 02:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Micro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.564100</td>\n",
       "      <td>0.116329</td>\n",
       "      <td>42.188739</td>\n",
       "      <td>44.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.131400</td>\n",
       "      <td>0.119828</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>48.148148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.113269</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>48.148148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.109572</td>\n",
       "      <td>44.197138</td>\n",
       "      <td>51.851852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.106036</td>\n",
       "      <td>43.201220</td>\n",
       "      <td>43.209877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.120900</td>\n",
       "      <td>0.107886</td>\n",
       "      <td>39.280360</td>\n",
       "      <td>50.617284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>0.106820</td>\n",
       "      <td>42.688679</td>\n",
       "      <td>48.148148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.105895</td>\n",
       "      <td>46.390642</td>\n",
       "      <td>46.913580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.105814</td>\n",
       "      <td>46.881196</td>\n",
       "      <td>46.913580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.106070</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.679012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/miniconda3/envs/semeval/lib/python3.9/site-packages/transformers/trainer.py:2172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=710, training_loss=0.1641433339723399, metrics={'train_runtime': 142.7142, 'train_samples_per_second': 39.52, 'train_steps_per_second': 4.975, 'total_flos': 2585440475504640.0, 'train_loss': 0.1641433339723399, 'epoch': 10.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 9: 設置模型和訓練器\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(args[\"model_name\"])\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Cell 10: 開始訓練\n",
    "print(\"開始訓練...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd6c91dd-75c3-4bea-a62f-2e99e122da96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加載測試數據...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae25d85ba84a4df389d3dc2680f6bd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 11: 定義預測函數\n",
    "def get_predict(model, tokenized_dataset, batch_size=8, max_new_tokens=25, sample_set='test', device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    outputs = []\n",
    "    outputs_raw = []\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        tokenized_dataset[sample_set], \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        input_ids = [torch.tensor(example['input_ids']) for example in batch]\n",
    "        attention_mask = [torch.tensor(example['attention_mask']) for example in batch]\n",
    "        \n",
    "        # 找出最大長度\n",
    "        max_len = max(len(ids) for ids in input_ids)\n",
    "        \n",
    "        # 填充到最大長度\n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            padding_len = max_len - len(ids)\n",
    "            padded_input_ids.append(torch.cat([ids, torch.ones(padding_len, dtype=torch.long) * tokenizer.pad_token_id]))\n",
    "            padded_attention_mask.append(torch.cat([mask, torch.zeros(padding_len, dtype=torch.long)]))\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.stack(padded_input_ids),\n",
    "            'attention_mask': torch.stack(padded_attention_mask)\n",
    "        }\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        tokenized_dataset[sample_set], \n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn  # 使用自定義的 collate_fn\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_tokens = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "            \n",
    "            decoded = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            outputs_raw.extend(decoded)\n",
    "            \n",
    "            # 轉換為二進制標籤\n",
    "            batch_outputs = [0 if out.startswith(\"Option 1\") else 1 for out in decoded]\n",
    "            outputs.extend(batch_outputs)\n",
    "            \n",
    "    return outputs, outputs_raw\n",
    "\n",
    "# Cell 12: 加載測試數據\n",
    "print(\"加載測試數據...\")\n",
    "test_data = read_jsonl(args[\"data_test_pth\"])\n",
    "test_dict = trans_to_dict_qqa(test_data[0])\n",
    "test_dataset = Dataset.from_dict(test_dict)\n",
    "\n",
    "# 對測試數據進行預處理\n",
    "test_tokenized = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=test_dataset.column_names,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0505391-4714-43b6-b390-d10bbb803bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始預測...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 21/21 [00:01<00:00, 13.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "測試集評估結果：\n",
      "Accuracy: 50.62%\n",
      "Macro F1: 42.14%\n",
      "Micro F1: 50.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: 進行預測和評估\n",
    "print(\"開始預測...\")\n",
    "decoded_preds, decoded_preds_raw = get_predict(\n",
    "    model=model,\n",
    "    tokenized_dataset={\"test\": test_tokenized},\n",
    "    batch_size=8,\n",
    "    max_new_tokens=25\n",
    ")\n",
    "\n",
    "labels = [0 if ans.startswith(\"Option 1\") else 1 for ans in test_dict['answer']]\n",
    "\n",
    "# 計算評估指標\n",
    "# f1_metric = evaluate.load(\"./f1.py\")\n",
    "macro_f1 = f1_metric.compute(predictions=decoded_preds, references=labels, average=\"macro\")\n",
    "micro_f1 = f1_metric.compute(predictions=decoded_preds, references=labels, average=\"micro\")\n",
    "\n",
    "# 計算準確率\n",
    "accuracy = sum(1 for p, t in zip(decoded_preds, labels) if p == t) / len(labels)\n",
    "\n",
    "print(\"\\n測試集評估結果：\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Macro F1: {macro_f1['f1']*100:.2f}%\")\n",
    "print(f\"Micro F1: {micro_f1['f1']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02fb146a-21fc-4adb-beef-849d75c21f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "保存預測結果到：./results/predictions_qqa.json\n"
     ]
    }
   ],
   "source": [
    "save_res = []\n",
    "for q, o1, o2, ans, pred, pred_raw in zip(\n",
    "    test_dict['question'],\n",
    "    test_dict['Option1'],\n",
    "    test_dict['Option2'],\n",
    "    test_dict['answer'],\n",
    "    decoded_preds,\n",
    "    decoded_preds_raw\n",
    "):\n",
    "    save_res.append({\n",
    "        \"question\": q,\n",
    "        \"option1\": o1,\n",
    "        \"option2\": o2,\n",
    "        \"answer\": ans,\n",
    "        \"prediction\": f\"Option {pred+1}\",\n",
    "        \"model_output\": pred_raw\n",
    "    })\n",
    "\n",
    "output_dir = \"./results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"predictions_qqa.json\")\n",
    "\n",
    "print(f\"\\n保存預測結果到：{output_path}\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(save_res, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdddbc34-86e6-4f37-bc6d-e79647c2ceaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
